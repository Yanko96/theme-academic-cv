<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Deep Learning | Yangzhe Kong</title>
    <link>http://localhost:1313/tags/deep-learning/</link>
      <atom:link href="http://localhost:1313/tags/deep-learning/index.xml" rel="self" type="application/rss+xml" />
    <description>Deep Learning</description>
    <generator>Hugo Blox Builder (https://hugoblox.com)</generator><language>en-us</language><lastBuildDate>Thu, 31 Mar 2022 00:00:00 +0000</lastBuildDate>
    <image>
      <url>http://localhost:1313/media/icon_hu7729264130191091259.png</url>
      <title>Deep Learning</title>
      <link>http://localhost:1313/tags/deep-learning/</link>
    </image>
    
    <item>
      <title>Point Cloud-based TNT for Trajectory Prediction</title>
      <link>http://localhost:1313/project/point_cloud_based_tnt/</link>
      <pubDate>Thu, 31 Mar 2022 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/project/point_cloud_based_tnt/</guid>
      <description>&lt;p&gt;This project was done at research team of perception team at ADAS, Huawei Technologies Co., Ltd.&lt;/p&gt;
&lt;h2 id=&#34;motivation&#34;&gt;Motivation&lt;/h2&gt;
&lt;p&gt;Although most of the autonomous driving algorithm follows the pipeline of &amp;ldquo;detection-tracking-prediction&amp;rdquo;, it can be deficient since each stage will accumulate the errors previous stages have made. In some cases, the errors will be amplified and result in disastrous accidents. Moreover, since in later stages, the features that previous stages used to produce the results are not available anymore, which makes correcting erroneous outputs infeasible. Therefore, instead of exploiting rule-based sensor fusion and the sequential pipeline &amp;ldquo;detection-tracking-prediction&amp;rdquo;, our research team decided to explore Hydranet-like networks that fuse features, detect objects, track objects and predict object trajectory. Specifically, detecting, tracking and predicting are completed with different heads that connected to the same backbone network that fuses features from different sensors.&lt;/p&gt;
&lt;p&gt;To testify the feasibility, we want to verify if it&amp;rsquo;s doable to predict trajectory based on raw point cloud data. And that&amp;rsquo;s how this project came into being.&lt;/p&gt;
&lt;h2 id=&#34;model-structure&#34;&gt;Model Structure&lt;/h2&gt;
&lt;h3 id=&#34;backbone-structure&#34;&gt;Backbone Structure&lt;/h3&gt;
&lt;p&gt;We adopt the detection model from detection team. It&amp;rsquo;s a UNet-like model and its detection head resembles CenterPoint. A lot of somplifications have been done to accelerate the model and make it compatible for AI chips on vehicles. One thing worth noting is that, instead of binary voxels, the model divides the space into fixed-size voxels and compute 4 features for each voxel: Max Z, Min Z, Mean Intensity, No. points.&lt;/p&gt;
&lt;h3 id=&#34;tnt&#34;&gt;TNT&lt;/h3&gt;
&lt;p&gt;We first crop rasterized HDMap and feature maps from detection backbone, according to the obstacle position. Then we merge the crop from HDMap and the crop from detection backbone feature maps, to generate the feature for each obstacle, which is then feeded into TNT. The 3 stages of TNT remain unchanged.&lt;/p&gt;
&lt;h3 id=&#34;overall-structure&#34;&gt;Overall Structure&lt;/h3&gt;
&lt;p&gt;Here&amp;rsquo;s a figure that illustrates the overall structure of the point cLoud-based TNT.&lt;/p&gt;
&lt;div align=center&gt;&lt;img src=&#34;structure.svg&#34; width=600&gt;&lt;/div&gt;
&lt;h2 id=&#34;dataset&#34;&gt;Dataset&lt;/h2&gt;
&lt;p&gt;Since we use point cloud dataset, there&amp;rsquo;s a huge problem. The distribution of trajectories in the dataset is ill-posed. More than 20% of obstacles in the dataset are either going straight or staying still. To tackle this problem, we use 3 tricks:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;resample the dataset: upsample difficult scenarios and downsample easy ones.&lt;/li&gt;
&lt;li&gt;use differentiated loss: higher coefficients for difficult scenarios and lower for easy ones.&lt;/li&gt;
&lt;li&gt;do data augmentation: flip, rotate, etc.&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;demo&#34;&gt;Demo&lt;/h2&gt;
&lt;p&gt;I&amp;rsquo;m very sorry that due to the regulation of Huawei, I cannot show any source code or finished effect of the model. These demo videos are the only things that I can provide here.&lt;/p&gt;
&lt;!-- &lt;iframe height=1000 width=1000 src=&#34;demo_1.avi&#34;&gt;

&lt;iframe height=1000 width=1000 src=&#34;demo_2.avi&#34;&gt;

&lt;iframe height=1000 width=1000 src=&#34;demo_3.avi&#34;&gt;

&lt;iframe height=1000 width=1000 src=&#34;demo_4.avi&#34;&gt; --&gt;
&lt;video width=&#34;1000&#34; height=&#34;1000&#34; controls&gt;
  &lt;source src=&#34;demo_1.mp4&#34; type=&#34;video/mp4&#34;&gt;
&lt;/video&gt;
&lt;video width=&#34;1000&#34; height=&#34;1000&#34; controls&gt;
  &lt;source src=&#34;demo_2.mp4&#34; type=&#34;video/mp4&#34;&gt;
&lt;/video&gt;
&lt;video width=&#34;1000&#34; height=&#34;1000&#34; controls&gt;
  &lt;source src=&#34;demo_3.mp4&#34; type=&#34;video/mp4&#34;&gt;
&lt;/video&gt;
&lt;video width=&#34;1000&#34; height=&#34;1000&#34; controls&gt;
  &lt;source src=&#34;demo_4.mp4&#34; type=&#34;video/mp4&#34;&gt;
&lt;/video&gt;
&lt;h2 id=&#34;conclusion&#34;&gt;Conclusion&lt;/h2&gt;
&lt;p&gt;Due to the volumn of point cloud dataset, it contains far less scenarios than ordinary trajectory prediction dataset. However, the performance is considerable as most peaks of multi-modal predictions are correctly assigned. We belive there&amp;rsquo;s still lots of space for improvement, if we use larger dataset and add &lt;strong&gt;interaction module&lt;/strong&gt; in the model.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Learning to Solve Soft-Constrained Vehicle Routing Problems with Lagrangian Relaxation</title>
      <link>http://localhost:1313/project/learn_cvrptw/</link>
      <pubDate>Mon, 31 May 2021 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/project/learn_cvrptw/</guid>
      <description>&lt;p&gt;The project was done at Huawei Technologies Co., Ltd., and is vailable on Arxiv.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Tang Q, Kong Y, Pan L, Lee C. Learning to Solve Soft-Constrained Vehicle Routing Problems with Lagrangian Relaxation. arXiv preprint arXiv:2207.09860. 2022 Jul 20. &lt;/cite&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Paper [
]&lt;/p&gt;
&lt;h2 id=&#34;motivation&#34;&gt;Motivation&lt;/h2&gt;
&lt;p&gt;Exact algorithms is not always computationally efficient enough to deploy, especially on large scenarios with complex constraints. Other methods like heuristics or meta-heuristics solvers are also faced with the same dilemma. Vehicle Routing Problems (VRPs) in real-world applications often come with various constraints, therefore bring additional computational challenges to classical algorithms like exact solution methods or heuristic search approaches. The recent idea to learn heuristic move patterns from sample data has become increasingly promising to reduce solution developing costs. However, using learning-based approaches to address more types of constrained VRP remains a challenge.&lt;/p&gt;
&lt;h2 id=&#34;trajectory-shaping&#34;&gt;Trajectory Shaping&lt;/h2&gt;
&lt;p&gt;We improve the model performance by intervening the trajectory generation process to boost the quality of the agent’s training information. The motivation is similar to modifying the expression of return. Due to the large search space and the sparsity of optima, guiding the agent to explore and learn the ’good’ actions can be very slow or easily trapped into local optima, especially if the initial state solution is far from the true global optimum. With the underlying model being deterministic and we can easily obtain the next state&amp;rsquo;s reward and cost, we suggest a post-action rejection rule deciding whether to reject the candidate solution respectively when non-improved and improved solutions are found to modify the generated trajectories.&lt;/p&gt;
&lt;img src=&#34;https://latex.codecogs.com/svg.image?&amp;space;&amp;space;&amp;space;&amp;space;P(\textnormal{Reject})&amp;space;=&amp;space;\left\{&amp;space;&amp;space;&amp;space;&amp;space;&amp;space;&amp;space;&amp;space;&amp;space;\begin{array}{ll}&amp;space;&amp;space;&amp;space;&amp;space;&amp;space;&amp;space;&amp;space;&amp;space;&amp;space;&amp;space;&amp;space;&amp;space;\phi&amp;space;&amp;&amp;space;\quad&amp;space;\textnormal{if&amp;space;improved}&amp;space;\\&amp;space;&amp;space;&amp;space;&amp;space;&amp;space;&amp;space;&amp;space;&amp;space;&amp;space;&amp;space;&amp;space;&amp;space;1&amp;space;-&amp;space;\phi&amp;space;&amp;&amp;space;\quad&amp;space;\textnormal{if&amp;space;not&amp;space;improved},&amp;space;&amp;space;&amp;space;&amp;space;&amp;space;&amp;space;&amp;space;&amp;space;\end{array}&amp;space;&amp;space;&amp;space;&amp;space;\right.&#34; title=&#34;https://latex.codecogs.com/svg.image? P(\textnormal{Reject}) = \left\{ \begin{array}{ll} \phi &amp; \quad \textnormal{if improved} \\ 1 - \phi &amp; \quad \textnormal{if not improved}, \end{array} \right.&#34; /&gt;
&lt;div align=center&gt;&lt;img src=&#34;tc_not_tc.png&#34; width=&#34;800&#34;&gt;&lt;/div&gt;
&lt;h2 id=&#34;modified-return&#34;&gt;Modified Return&lt;/h2&gt;
&lt;p&gt;The expression of $G_{t}$ is specially designed to encourage better performance in soft-constrained VRPs with 2-exchange moves. First, the immediate reward is penalized by the immediate cost such that the agent is encouraged to find better moves while balancing the reward and cost with iteratively updated $\lambda$s. In addition, We calculate the cumulative value using the maximum value of all pairs of subsequent moves from $s_{t}$ to $s_{t&#39;}$ instead of a summation over all consecutive moves from $s_{t}$ to $s_{t+1}$ as in the $Return$ definition. &amp;ldquo;Bad&amp;rdquo; operations that do not improve the objective function will be suppressed, while only the &amp;lsquo;good&amp;rsquo; actions are rewarded with the $\max$ function. It also tends to decorrelate the effect of a series of historical operations so that the agent is less affected by locally optimal trajectories. To sum up, we apply such modification to better mimic the heuristic search process by encouraging more immediate and effective actions that improve the cost-penalized objective function. The following figure provides a visual representation of the definition of $G_t$.&lt;/p&gt;
&lt;div align=center&gt;&lt;img src=&#34;return.png&#34; width=&#34;800&#34;&gt;&lt;/div&gt;
&lt;div align=center&gt;&lt;img src=&#34;return_vs.png&#34; width=&#34;800&#34;&gt;&lt;/div&gt;
&lt;h2 id=&#34;performance&#34;&gt;Performance&lt;/h2&gt;
&lt;p&gt;We observed slightly better performance than Google OR-Tools and close performance to LKH-3.&lt;/p&gt;
&lt;div align=center&gt;&lt;img src=&#34;perf.PNG&#34; width=&#34;800&#34;&gt;&lt;/div&gt;
&lt;h2 id=&#34;concerns-on-the-dataset&#34;&gt;Concerns on the dataset&lt;/h2&gt;
&lt;p&gt;Although generation of VRP/CVRP datasets is pretty intuitive, VRPTW datasets are tricky to deal with. In our implementation we generate first a CVRP scenario and then a CVRP solution by heuristics. Time windows are then generated according to arrival time in the CVRP solution to make sure that there is at least one valid sulution. However, we believe that there are better ways to generate VRPTW/CVRPTW datsaets.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Path-Link Graph Neural Network for IP Network Performance Prediction</title>
      <link>http://localhost:1313/project/plnet/</link>
      <pubDate>Fri, 31 Jul 2020 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/project/plnet/</guid>
      <description>&lt;!-- # Path-Link-Graph-Nerural-Network-for-IP-Performance-Prediction --&gt;
&lt;p&gt;The project was done in Nokia Bell Labs. The paper Path-Link Graph Neural Network for IP Network Performance Prediction is published in 2021 IFIP/IEEE International Symposium on Integrated Network Management (IM).&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Kong, Y., Petrov, D., Räisänen, V. and Ilin, A., 2021, May. &lt;cite&gt; Path-Link Graph Neural Network for IP Network Performance Prediction. In 2021 IFIP/IEEE International Symposium on Integrated Network Management (IM) (pp. 170-177). IEEE. &lt;/cite&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Paper [
]&lt;/p&gt;
&lt;h2 id=&#34;motivation&#34;&gt;Motivation&lt;/h2&gt;
&lt;p&gt;Dynamic resource provisioning and quality assurance for the plethora of end-to-end slices running over 5G and B5G networks require advanced modeling capabilities. Graph Neural Networks (GNN) have already proven their efficiency for network performance prediction. We verified a SOTA model RouteNet by a new
implementation in the PyTorch ML library. Next, with the aims to improve accuracy and scalability, an alternative Path-Link neural network (PLNet) architecture is proposed and evaluated.&lt;/p&gt;
&lt;h2 id=&#34;performance&#34;&gt;Performance&lt;/h2&gt;
&lt;p&gt;We observed slightly better accuracy and better generalization.&lt;/p&gt;
&lt;div align=center&gt;&lt;img src=&#34;perf_train.png&#34; width=&#34;800&#34;&gt;&lt;/div&gt;
&lt;div align=center&gt;&lt;img src=&#34;perf_test.png&#34; width=&#34;1000&#34;&gt;&lt;/div&gt;
&lt;h2 id=&#34;improved-scalability&#34;&gt;Improved Scalability&lt;/h2&gt;
&lt;p&gt;Largely improved scalability is observed.&lt;/p&gt;
&lt;div align=center&gt;&lt;img src=&#34;scalability.png&#34;&gt;&lt;/div&gt;
&lt;h2 id=&#34;future-works&#34;&gt;Future Works&lt;/h2&gt;
&lt;p&gt;There are several future directions for the continuation of this study. Firstly, RouteNet and PLNet models have good potential for reinforcement learning. For example, dynamic resource allocation. Secondly, although we consider more generic scenarios than 5G in this paper, it is
still a good starting point for going further into more specific
5G scenarios. That is to say, extending the comparison and application of the models on more extensive networks and in the context of 5G scenarios like end-to-end slicing are also promising research topics.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>A2C Agent for playing Wimblepong with pretrained VAE as the encoder</title>
      <link>http://localhost:1313/project/a2c_pretrain/</link>
      <pubDate>Mon, 16 Dec 2019 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/project/a2c_pretrain/</guid>
      <description>&lt;p&gt;This project served as the final project of course
ELEC-E8125&amp;ndash;Reinforcement-learning D. The code is available 
&lt;/p&gt;
&lt;p&gt;Wimblepong is a two player version of the pong-v0 OpenAI Gym environment developed by Intelligent Robotics group at Aalto University, Finland.&lt;/p&gt;
&lt;h2 id=&#34;motivation&#34;&gt;Motivation&lt;/h2&gt;
&lt;p&gt;In this project, we were asked to develop an agent for wimblepong and the agents will be tested in a battle royale. In addition, we have 2 options for state space: the visual observation or the encoded vector of the state. Altough many classmates chose to clone github repos of SOTA algorithms such as TRPO, PPO and Dueling Deep Q Networks, I decided to challenge myself and verify one of my questions: Will supervised pretained models help accelerate divergence of reinforcement learning agents? Therefore, I chose to use visual observations and first train a VAE to encode the visual observation, then train an A2C agent of which the input is the encoded state from the VAE encoder.&lt;/p&gt;
&lt;p&gt;For sure A2C cannot be better than fancier algorithms, I&amp;rsquo;m still proud of myself, for bringing up ideas and verifying them independently.&lt;/p&gt;
&lt;h2 id=&#34;pretrained-cnn-vae&#34;&gt;Pretrained CNN-VAE&lt;/h2&gt;
&lt;p&gt;A CNN-VAE is pre-trained on collected observations of the wimblepong environment in order to accelerate the converge of the agent training. The VAE adopts a similar model strcture as ResNet. Some of the results on the test set are shown below.&lt;/p&gt;
&lt;img src=&#34;reconstructed_0.png&#34; width=&#34;800&#34;&gt;
&lt;img src=&#34;reconstructed_1.png&#34; width=&#34;800&#34;&gt;
&lt;img src=&#34;reconstructed_3.png&#34; width=&#34;800&#34;&gt;
&lt;img src=&#34;reconstructed_4.png&#34; width=&#34;800&#34;&gt;
&lt;h2 id=&#34;a2c-agent&#34;&gt;A2C Agent&lt;/h2&gt;
&lt;p&gt;The encoder of the Agent is loaded from the checkpoint of the encoder of the pre-trained CNN-VAE. Then the agent is trained by A2C algorithm with entropy loss to encourage exploration. With pre-trained VAE loaded as the encoder, the convergence of the agent is accelerated as the following figures show (green paddle is the agent).&lt;/p&gt;
&lt;div align=center&gt;&lt;img src=&#34;return_vs.png&#34; width=600&gt;&lt;/div&gt;
&lt;div align=center&gt;&lt;img src=&#34;win_rate_vs.png&#34; width=600&gt;&lt;/div&gt;
&lt;div align=center&gt;&lt;img src=&#34;replay.gif&#34;&gt;&lt;/div&gt;
&lt;h2 id=&#34;conclusion&#34;&gt;Conclusion&lt;/h2&gt;
&lt;p&gt;The pretrained encoder did help accelerate the convergence. However, there are several reasons why I don&amp;rsquo;t recommend doing so:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;There&amp;rsquo;s a big gap between reconstructing the observations and predicting reliable actions and q-values. This makes pretrained model not completely plug-and-play for RL tasks. I spent many efforts selecting most suitable checkpoints and learning rates. It&amp;rsquo;s not so worthwhile, especially considering that it only accelerate a relatively small amount of training time, but can hardly boost the performance.&lt;/li&gt;
&lt;li&gt;The model structure of VAE is not necessarily the best for RL models.&lt;/li&gt;
&lt;li&gt;Exploration is the most crucial for RL. Not these tricks (that are not helpful for exploration).&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Anyways, it&amp;rsquo;s still an interesting experience for me.&lt;/p&gt;
</description>
    </item>
    
  </channel>
</rss>
